"""

Automatically generated by Colaboratory. The visualization code here
will only work in an IPython notebook (see `%matplotlib inline`).

"""

import torch
import torchvision

import random
import numpy as np

torch.set_default_tensor_type('torch.cuda.FloatTensor')

class LeConv(torchvision.models.inception.BasicConv2d):
    def forward(self, x):
        x = self.conv(x)
        self.last_conv = x
        x = self.bn(x)
        return torch.nn.functional.relu(x, inplace=True)

torchvision.models.inception.BasicConv2d = LeConv

# For if you'd like to try and use resnet50 for the loss function.
class LeBottle(torchvision.models.resnet.Bottleneck):
  def forward(self, x):
    identity = x
    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)
    out = self.conv2(out)
    out = self.bn2(out)
    out = self.relu(out)
    out = self.conv3(out)
    self.last_conv = out
    out = self.bn3(out)
    if self.downsample is not None:
        identity = self.downsample(x)
    out += identity
    out = self.relu(out)
    return out

torchvision.models.resnet.Bottleneck = LeBottle

ident = torch.tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]])
sobelX = torch.tensor([[-3.0,0.0,3.0],[-10.0,0.0,10.0],[-3.0,0.0,3.0]])
sobelY = sobelX.T
laplacian = torch.tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])
def circular_pad(x, p):
  x = torch.cat((x[...,-p:,:],x,x[...,:p,:]),dim=-2)
  x = torch.cat((x[...,-p:],x,x[...,:p]), dim=-1)
  return x
def perchannel_conv(x, filters):
  b, ch, h, w = x.shape
  x = x.reshape(b*ch, 1, h, w)
  # Pad circular
  x = circular_pad(x,1)
  x = torch.nn.functional.conv2d(x, filters[:,None])
  return x.reshape(b, -1, h, w)
def perception(x):
  filters = torch.stack([ident, sobelX, sobelY, laplacian])
  return perchannel_conv(x, filters)

class CA(torch.nn.Module):
  def __init__(self, channels=18, hiddenWidth=256):
    super().__init__()
    self.channels = channels
    self.l1 = torch.nn.Conv2d(channels*4, hiddenWidth, 1)
    self.l2 = torch.nn.Conv2d(hiddenWidth, channels, 1)
    self.l2.weight.data.zero_()
  def forward(self, x):
    xp = perception(x)
    xp = self.l2(self.l1(xp).relu())
    b,c,w,h = xp.shape
    update_mask = torch.rand(b,1,w,h)>0.5
    return x + xp*update_mask
  def seed(self, n, sz=128):
    return torch.zeros(n,self.channels,sz,sz)

from torchvision.models import inception_v3

def loss_fn(ch):
    net = inception_v3(weights='DEFAULT')
    net.eval()
    def loss(x):
        net(x)
        activation = net.Mixed_6b.branch_pool.last_conv[:,ch,:,:].mean()
        return -activation
    return loss

color = torch.tensor([155, 150, 176]) / 255

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pylab as plt
from IPython.display import clear_output, display

mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])

class Viz:
    def __init__(self):
        self.lossHistory = []
        plt.ion()
        _, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))
        self.ax1 = ax1
        self.ax1.set_yscale('symlog')
        self.sc = self.ax1.scatter([],[],color='blue', alpha=0.5)
        self.ax2 = ax2
        self.tickCount = 0

    def tick(self, loss, x):
        self.tickCount += 1
        self.lossHistory.append(loss.cpu().numpy())
        if self.tickCount % 5 == 0:
            self.scatter()
            self.image(x)
            clear_output(wait=True)
            display(plt.gcf())

    def scatter(self):
        x = range(len(self.lossHistory))
        self.sc.set_offsets(list(zip(x, self.lossHistory)))
        self.ax1.set_xlim(min(x)-1, max(x)+1)
        self.ax1.set_ylim(min(self.lossHistory)-1, max(self.lossHistory)+1)

    def image(self,x):
        x = torch.cat(torch.unbind(x, dim=0), dim=2)
        x = x[1,:,:] * color.reshape(3,1,1)
        x = x.cpu().numpy().transpose(1,2,0) * std + mean
        self.ax2.imshow(x)
        self.ax2.axis('off')

def train():
    loss = loss_fn(137)
    bs = 5
    ch = 14
    ca = CA()

    # state_dict = torch.load("model_6b_137.pth")
    # ca.load_state_dict(state_dict)

    opt = torch.optim.Adam(ca.parameters(), 3e-3)
    lr_sched = torch.optim.lr_scheduler.MultiStepLR(opt, [1000, 2000], 0.3)
    viz = Viz()
    with torch.no_grad():
        pool = ca.seed(256)
    for trainingStep in range(2000):
        with torch.no_grad():
            batch_idx = np.random.choice(len(pool), bs, replace=False)
            x = pool[batch_idx]
            if trainingStep%8==0:
                x[:1] = ca.seed(1)
        steps = random.randint(32, 96)
        for _ in range(steps):
            x = ca(x)
        xp = x[:,:1,:,:]
        xp = xp * color.reshape(1,3,1,1)
        # using abs instead of square on 6c_40 the model gets to -infinity error.
        clipLoss = (xp).clip(0,1).sub(xp).square().sum() * 1e-4
        activation_loss = loss(xp)
        error = activation_loss + clipLoss
        with torch.no_grad():
            viz.tick(error,x)
            error.backward()
            for p in ca.parameters():
                p.grad /= (p.grad.norm()+1e-8)
            opt.step()
            opt.zero_grad()
            lr_sched.step()
            pool[batch_idx] = x
            if trainingStep % 10 == 0:
                torch.save(ca.state_dict(), "model_6b_137.pth")
    return ca

ca = train()
